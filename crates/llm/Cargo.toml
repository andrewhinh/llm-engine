[package]
name = "llm"
version.workspace = true
authors.workspace = true
edition.workspace = true
license.workspace = true
repository.workspace = true

[dependencies]
anyhow = "1.0.101"
attention-rs = { git = "https://github.com/guoqingbao/attention.rs.git", version = "0.4.1", rev = "2667902", optional = true, default-features = false }
axum = "0.8.4"
candle-core = "0.9.2"
candle-nn = "0.9.2"
hf-hub = "0.4.3"
parking_lot = "0.12.5"
rand = "0.10.0"
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.149"
sysinfo = "0.38.2"
thiserror = "2.0.18"
tokenizers = "0.22.2"
tokio = { version = "1.49.0", features = [
    "macros",
    "rt-multi-thread",
    "sync",
    "net",
    "time",
    "signal",
] }
tokio-stream = "0.1.17"
tower-http = { version = "0.6.6", features = ["cors"] }
tracing = "0.1.44"
tracing-subscriber = "0.3.22"

[features]
default = []
flash-attn = [
    "dep:attention-rs",
    "attention-rs/cuda",
    "attention-rs/flash-attn",
    "attention-rs/flash-decoding",
]
flashinfer = [
    "dep:attention-rs",
    "attention-rs/cuda",
    "attention-rs/flashinfer",
]
